{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iffathsaleem/Customer-Churn-Prediction-at-Telco/blob/main/Model_building.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import Necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "TQJqw9GkMfG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Scalling**"
      ],
      "metadata": {
        "id": "KB5ccVORgVNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Test**"
      ],
      "metadata": {
        "id": "HqKChufienlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Access the dataset \"clean_test\"\n",
        "clean_test = pd.read_csv(\"clean_test.csv\")"
      ],
      "metadata": {
        "id": "RCIvBrUIMgYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_test"
      ],
      "metadata": {
        "id": "sCBLTrnNMumt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using the robust scaller since our data is skewed and luxury sales has outliers. Since this technique scales using the median and IQR it is best suited to deal with the skewness and the extreme values."
      ],
      "metadata": {
        "id": "BG9JmpctgN0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "# Identify numerical columns\n",
        "numerical_columns = ['luxury_sales', 'fresh_sales', 'dry_sales']\n",
        "\n",
        "# Initialize RobustScaler\n",
        "DSPL_scaler = RobustScaler()\n",
        "\n",
        "# Fit on training numerical data and transform both train & test\n",
        "clean_test[numerical_columns] = DSPL_scaler.fit_transform(clean_test[numerical_columns])\n",
        "clean_test"
      ],
      "metadata": {
        "id": "kHyGdziSfhCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train**"
      ],
      "metadata": {
        "id": "dPg4jIoSe-RB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Access the dataset \"clean_train\"\n",
        "clean_train = pd.read_csv(\"clean_train.csv\")"
      ],
      "metadata": {
        "id": "a8O1lPJbSAKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_train"
      ],
      "metadata": {
        "id": "fKcmSFcDM2pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using the robust scaller since our data is skewed and luxury sales has outliers. Since this technique scales using the median and IQR it is best suited to deal with the skewness and the extreme values."
      ],
      "metadata": {
        "id": "Jc4_iECicDHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "# Identify numerical columns\n",
        "numerical_columns = ['luxury_sales', 'fresh_sales', 'dry_sales']\n",
        "\n",
        "# Initialize RobustScaler\n",
        "DSPL_scaler = RobustScaler()\n",
        "\n",
        "# Fit on training numerical data and transform both train & test\n",
        "clean_train[numerical_columns] = DSPL_scaler.fit_transform(clean_train[numerical_columns])\n",
        "clean_train"
      ],
      "metadata": {
        "id": "0jll3dL6RT54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "ToaLnlmgNNtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "5CJ5jRl8Mu7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define X and y variables\n",
        "X = clean_train[['luxury_sales', 'fresh_sales', 'dry_sales', 'outlet_city_encoded']]\n",
        "y = clean_train['cluster_catgeory']"
      ],
      "metadata": {
        "id": "CJnb1Kn7N6X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "j0tPgX93N3iU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train Logistic Regression model\n",
        "log_reg_model = LogisticRegression(max_iter=1000, solver='saga', random_state=42)\n",
        "log_reg_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "nOqhWYw8M5Fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Predict using test data\n",
        "y_pred = log_reg_model.predict(X_test)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "uOVvi9YbOJ9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "znGc09BzM445"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wHk5giZsM4xG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Report\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "FFoTnMVDM9DC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}